{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fca72d9-a811-4130-adbd-501192294320",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Linear Algebra module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b59b88-873f-47a0-9b77-9dd1111878d2",
   "metadata": {},
   "source": [
    "\n",
    "## Vectors and Vector Operations\n",
    "\n",
    "A **vector** is a one-dimensional array of numbers. We can think of it as a point in space. Vectors are fundamental in machine learning and data science, as they are used to represent features, parameters, or even inputs and outputs of models.\n",
    "\n",
    "Common vector operations include:\n",
    "\n",
    "- **Vector addition**: The sum of two vectors of the same length is obtained by adding corresponding elements.\n",
    "- **Scalar multiplication**: Multiplying a vector by a scalar means multiplying each component by the scalar.\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "Given two vectors $\\textbf{v}$ and $\\textbf{w}$ of the same dimension:\n",
    "\n",
    "$$\n",
    "\\textbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}, \\quad \\textbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- **Vector addition**: The sum $\\textbf{v} + \\textbf{w}$ is defined as:\n",
    "\n",
    "$$\n",
    "\\textbf{v} + \\textbf{w} = \\begin{bmatrix} v_1 + w_1 \\\\ v_2 + w_2 \\\\ \\vdots \\\\ v_n + w_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- **Scalar multiplication**: If $c$ is a scalar and $\\textbf{v}$ is a vector, the scalar multiplication $c \\textbf{v}$ is given by:\n",
    "\n",
    "$$\n",
    "c \\textbf{v} = \\begin{bmatrix} c v_1 \\\\ c v_2 \\\\ \\vdots \\\\ c v_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Example 1: Basic Vector Operations\n",
    "\n",
    "Let $\\textbf{v} = [1, 2, 3]$ and $\\textbf{w} = [4, 5, 6]$. The sum $\\textbf{v} + \\textbf{w}$ is calculated by adding the corresponding components:\n",
    "\n",
    "$$\n",
    "\\textbf{v} + \\textbf{w} = \\begin{bmatrix} 1 + 4 \\\\ 2 + 5 \\\\ 3 + 6 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 7 \\\\ 9 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Example 2: Scalar Multiplication\n",
    "\n",
    "Now, if we take $c = 3$, the scalar multiplication $c \\textbf{v}$ is calculated as follows:\n",
    "\n",
    "$$\n",
    "3 \\textbf{v} = \\begin{bmatrix} 3 \\times 1 \\\\ 3 \\times 2 \\\\ 3 \\times 3 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 6 \\\\ 9 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Example 3: Use Case in Machine Learning\n",
    "\n",
    "In machine learning, vectors often represent features of a data point. For example, in image recognition, a pixel image could be represented as a vector of pixel values.\n",
    "\n",
    "Let's say we are dealing with a dataset of images, and each image is represented as a vector of pixel intensities. A common preprocessing step is to normalize the pixel values or apply transformations such as scalar multiplication to scale the vector.\n",
    "\n",
    "#### Additional Example: Vector Addition in a Data Science Context\n",
    "\n",
    "In collaborative filtering (a recommender system), vector addition can be used to combine user preferences (represented as vectors) with item features to predict user ratings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b1c0bf9-3255-4dfe-942f-35f92415ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector sum (v + w): [5 7 9]\n",
      "Scalar multiplication (3 * v): [3 6 9]\n",
      "Scaled pixel intensities: [0.2 0.4 0.6]\n",
      "Predicted rating (user + item): [5.5 3.2 0.8]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Example 1: Basic vector operations\n",
    "v = np.array([1, 2, 3])\n",
    "w = np.array([4, 5, 6])\n",
    "\n",
    "# Vector addition\n",
    "vector_sum = v + w\n",
    "print(\"Vector sum (v + w):\", vector_sum)\n",
    "\n",
    "# Scalar multiplication\n",
    "scalar_mult = 3 * v\n",
    "print(\"Scalar multiplication (3 * v):\", scalar_mult)\n",
    "\n",
    "# Example 2: Use case - Scaling pixel intensities in an image (represented as a vector)\n",
    "image_pixel_vector = np.array([0.1, 0.2, 0.3])  # Example pixel values\n",
    "scaled_pixel_vector = 2 * image_pixel_vector  # Scaling by a factor of 2\n",
    "print(\"Scaled pixel intensities:\", scaled_pixel_vector)\n",
    "\n",
    "# Example 3: Recommender system example (Collaborative filtering)\n",
    "user_preferences = np.array([5, 3, 0])  # User likes items 1 and 2, indifferent to item 3\n",
    "item_features = np.array([0.5, 0.2, 0.8])  # Item features based on historical data\n",
    "\n",
    "# Predicting a rating by adding user preferences and item features\n",
    "predicted_rating = user_preferences + item_features\n",
    "print(\"Predicted rating (user + item):\", predicted_rating)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b0a37c-6f77-45a9-b021-d1cbbf14c65e",
   "metadata": {},
   "source": [
    "\n",
    "## Dot Product\n",
    "\n",
    "The **dot product** of two vectors $ \\textbf{v} $ and $ \\textbf{w} $, both of the same length, is the sum of the products of corresponding entries. The dot product is useful in calculating projections, angles between vectors, and more.\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "Given two vectors:\n",
    "\n",
    "$$\n",
    "\\textbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}, \\quad \\textbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The dot product $ \\textbf{v} \\cdot \\textbf{w} $ is defined as:\n",
    "\n",
    "$$\n",
    "\\textbf{v} \\cdot \\textbf{w} = v_1 w_1 + v_2 w_2 + \\cdots + v_n w_n = \\sum_{i=1}^{n} v_i w_i\n",
    "$$\n",
    "\n",
    "### Example 1: Basic Dot Product Calculation\n",
    "\n",
    "Let $ \\textbf{v} = [1, 2, 3] $ and $ \\textbf{w} = [4, 5, 6] $. The dot product is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\textbf{v} \\cdot \\textbf{w} = (1)(4) + (2)(5) + (3)(6) = 4 + 10 + 18 = 32\n",
    "$$\n",
    "\n",
    "### Example 2: Geometrical Interpretation\n",
    "\n",
    "The dot product can also be used to calculate the angle between two vectors. The cosine of the angle between vectors $\\textbf{v}$ and $\\textbf{w}$ is given by:\n",
    "\n",
    "$$\n",
    "\\cos(\\theta) = \\frac{\\textbf{v} \\cdot \\textbf{w}}{\\|\\textbf{v}\\| \\|\\textbf{w}\\|}\n",
    "$$\n",
    "\n",
    "Where $\\|\\textbf{v}\\|$ is the magnitude of vector $\\textbf{v}$. This is widely used in computer vision and NLP to measure the similarity of data points.\n",
    "\n",
    "### Example 3: Use Case in Machine Learning\n",
    "\n",
    "In machine learning, the dot product is used in algorithms like linear regression and neural networks. For instance, in linear regression, the prediction is obtained by computing the dot product between feature vectors and weight vectors.\n",
    "\n",
    "### Example 4: Cosine Similarity\n",
    "\n",
    "In natural language processing (NLP), **cosine similarity** is a metric used to measure how similar two documents are based on their feature vectors. It uses the dot product to calculate the cosine of the angle between the vectors.\n",
    "\n",
    "#### Formula for Cosine Similarity:\n",
    "\n",
    "$$\n",
    "\\text{Cosine Similarity} = \\frac{\\textbf{v} \\cdot \\textbf{w}}{\\|\\textbf{v}\\| \\|\\textbf{w}\\|}\n",
    "$$\n",
    "\n",
    "This is frequently used in search engines, recommendation systems, and document clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7f02f42-2fb0-438b-a42a-56e02042ddf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product (v · w): 32\n",
      "Cosine of the angle between v and w: 0.9746318461970762\n",
      "Prediction (linear regression): 2.0\n",
      "Cosine similarity between doc1 and doc2: 0.7302967433402214\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Example 1: Basic dot product\n",
    "v = np.array([1, 2, 3])\n",
    "w = np.array([4, 5, 6])\n",
    "dot_product = np.dot(v, w)\n",
    "print(\"Dot product (v · w):\", dot_product)\n",
    "\n",
    "# Example 2: Calculating cosine of the angle between two vectors\n",
    "magnitude_v = np.linalg.norm(v)\n",
    "magnitude_w = np.linalg.norm(w)\n",
    "cos_theta = dot_product / (magnitude_v * magnitude_w)\n",
    "print(\"Cosine of the angle between v and w:\", cos_theta)\n",
    "\n",
    "# Example 3: Use case in machine learning - Dot product in linear regression\n",
    "# Assume feature vector x and weight vector w\n",
    "x = np.array([2, 3, 4])\n",
    "weights = np.array([0.5, 0.2, 0.1])\n",
    "prediction = np.dot(x, weights)\n",
    "print(\"Prediction (linear regression):\", prediction)\n",
    "\n",
    "# Example 4: Cosine similarity between two document vectors (example in NLP)\n",
    "doc1 = np.array([1, 2, 1])\n",
    "doc2 = np.array([0, 1, 2])\n",
    "cosine_similarity = np.dot(doc1, doc2) / (np.linalg.norm(doc1) * np.linalg.norm(doc2))\n",
    "print(\"Cosine similarity between doc1 and doc2:\", cosine_similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5ac32d-c779-452c-8efe-c2fcdba4a985",
   "metadata": {},
   "source": [
    "## Matrix Multiplication\n",
    "\n",
    "A **matrix** is a two-dimensional array of numbers arranged in rows and columns. **Matrix multiplication** is one of the most important operations in linear algebra, widely used in fields such as computer graphics, machine learning, physics, and economics.\n",
    "\n",
    "### Why Do We Need Matrix Multiplication?\n",
    "\n",
    "Matrix multiplication helps in solving systems of linear equations, transforming geometric objects (rotation, scaling), and modeling real-world phenomena. It’s essential for efficient computation in various algorithms, including those used in deep learning and optimization problems. Simple applications include:\n",
    "\n",
    "- **Transformation of coordinates** in graphics\n",
    "- **Linear systems** solutions in physics and engineering\n",
    "- **Data compression** and **encryption algorithms**\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "Matrix multiplication involves taking the **dot product** of the rows of the first matrix with the columns of the second matrix. For two matrices to be multiplied, the number of columns in the first matrix must equal the number of rows in the second matrix.\n",
    "\n",
    "Given an $m \\times n$ matrix $A$ and an $n \\times p$ matrix $B$, the resulting matrix $C$ will have dimensions $m \\times p$. Each element $ c_{ij}$ in the resulting matrix is the dot product of the $ i $-th row of $ A$ with the $ j$-th column of $B$:\n",
    "\n",
    "$$\n",
    "C = A \\cdot B, \\quad C_{ij} = \\sum_{k=1}^{n} A_{ik} \\cdot B_{kj}\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can compute the matrix multiplication \\( A \\cdot B \\) as follows:\n",
    "\n",
    "$$\n",
    "A \\cdot B = \\begin{bmatrix} (1)(5) + (2)(7) & (1)(6) + (2)(8) \\\\ (3)(5) + (4)(7) & (3)(6) + (4)(8) \\end{bmatrix}\n",
    "= \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Step-by-Step Breakdown\n",
    "\n",
    "- First row, first column: $ (1)(5) + (2)(7) = 5 + 14 = 19 $\n",
    "- First row, second column: $ (1)(6) + (2)(8) = 6 + 16 = 22 $\n",
    "- Second row, first column: $ (3)(5) + (4)(7) = 15 + 28 = 43 $\n",
    "- Second row, second column: $ (3)(6) + (4)(8) = 18 + 32 = 50 $\n",
    "\n",
    "Thus, the result of the matrix multiplication is:\n",
    "\n",
    "$$\n",
    "A \\cdot B = \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Applications in Real Life\n",
    "\n",
    "- **Machine learning**: Neural networks use matrix multiplication to propagate input data through layers of neurons.\n",
    "- **Computer graphics**: Matrices are used to rotate, scale, and translate objects in 3D space.\n",
    "- **Economics**: Matrices are used in input-output models to predict how industries affect each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d437c9cb-e2e2-4d23-a20f-9b3a8624c58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19, 22],\n",
       "       [43, 50]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define matrices\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# Matrix multiplication\n",
    "matrix_product = np.dot(A, B)\n",
    "matrix_product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00007d97-dc69-4e06-bd9a-ee1fa19e4d19",
   "metadata": {},
   "source": [
    "## Determinant of a Matrix\n",
    "\n",
    "The **determinant** of a square matrix is a scalar value that can be computed from its elements. The determinant plays a crucial role in linear algebra, as it provides important information about the matrix. It is particularly useful for understanding whether a matrix is invertible (non-zero determinant) or singular (zero determinant).\n",
    "\n",
    "### Why Do We Need the Determinant?\n",
    "\n",
    "- **Matrix Invertibility**: A matrix is invertible (has an inverse) if and only if its determinant is non-zero.\n",
    "- **Area/Volume Interpretation**: The determinant can represent the scaling factor by which a transformation (represented by the matrix) scales area (in 2D) or volume (in 3D). A zero determinant indicates that the transformation collapses the space into a lower dimension (e.g., a line or a point).\n",
    "- **Linear Independence**: The determinant provides insight into whether the rows or columns of a matrix are linearly independent. If the determinant is zero, the rows or columns are linearly dependent.\n",
    "- **Solving Systems of Linear Equations**: Determinants are used in Cramer's Rule to solve systems of linear equations.\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "For a square $n \\times n $ matrix, the determinant is a recursive function that sums products of the matrix’s elements and their minors. For small matrices, the determinant can be computed easily.\n",
    "\n",
    "#### 2x2 Matrix\n",
    "\n",
    "The determinant of a 2x2 matrix $ A $ is given by:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}, \\quad \\text{det}(A) = ad - bc\n",
    "$$\n",
    "\n",
    "#### 3x3 Matrix\n",
    "\n",
    "The determinant of a 3x3 matrix $ A $ is:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{det}(A) = a(ei - fh) - b(di - fg) + c(dh - eg)\n",
    "$$\n",
    "\n",
    "### Example: Determinant of a 2x2 Matrix\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The determinant is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{det}(A) = (1)(4) - (2)(3) = 4 - 6 = -2\n",
    "$$\n",
    "\n",
    "Since the determinant is non-zero, the matrix is invertible.\n",
    "\n",
    "### Example: Determinant of a 3x3 Matrix\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "B = \\begin{bmatrix} 2 & 3 & 1 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The determinant is:\n",
    "\n",
    "$$\n",
    "\\text{det}(B) = 2((5)(9) - (6)(8)) - 3((4)(9) - (6)(7)) + 1((4)(8) - (5)(7))\n",
    "$$\n",
    "\n",
    "Breaking it down:\n",
    "\n",
    "$$\n",
    "\\text{det}(B) = 2(45 - 48) - 3(36 - 42) + 1(32 - 35)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{det}(B) = 2(-3) - 3(-6) + 1(-3)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{det}(B) = -6 + 18 - 3 = 9\n",
    "$$\n",
    "\n",
    "Thus, $ \\text{det}(B) = 9 $, indicating that matrix $ B $ is invertible.\n",
    "\n",
    "### Applications in Real Life\n",
    "\n",
    "- **Physics**: Determinants help describe transformations in space, such as rotations and scaling in mechanics.\n",
    "- **Computer graphics**: Determinants are used in rendering techniques for determining object transformations and perspective.\n",
    "- **Engineering**: Determinants are used to analyze systems of equations representing physical phenomena like electrical circuits, structural models, and mechanical systems.\n",
    "\n",
    "Understanding the determinant allows us to assess the properties of matrices and apply them in various fields effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4bba3ef6-7125-4319-87bf-0c41a943565e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-2.0000000000000004)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determinant of a matrix\n",
    "determinant_A = np.linalg.det(A)\n",
    "determinant_A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c91ce1-3629-472d-a5ab-0945b0c43043",
   "metadata": {},
   "source": [
    "## Inverse of a Matrix\n",
    "\n",
    "The **inverse** of a square matrix $ A $ is denoted as $ A^{-1} $, and it's the matrix that satisfies the equation:\n",
    "\n",
    "$$\n",
    "A \\cdot A^{-1} = A^{-1} \\cdot A = I\n",
    "$$\n",
    "\n",
    "where $ I $ is the **identity matrix**, a matrix with 1's on the diagonal and 0's elsewhere. The identity matrix acts as the neutral element in matrix multiplication, just like 1 in scalar multiplication. Not all matrices have an inverse, and a matrix must have a **non-zero determinant** to be invertible.\n",
    "\n",
    "### Why Do We Need the Inverse of a Matrix?\n",
    "\n",
    "The inverse of a matrix is critical for solving systems of linear equations, finding transformations, and performing other operations in various fields like machine learning, computer graphics, and engineering:\n",
    "\n",
    "- **Solving Linear Systems**: If $ A \\textbf{x} = \\textbf{b} $, then the solution can be found by multiplying both sides by $ A^{-1}$, yielding $ \\textbf{x} = A^{-1} \\textbf{b} $.\n",
    "- **Matrix Division**: Since there is no direct \"division\" for matrices, we use the inverse instead.\n",
    "- **Linear Transformations**: Inverse matrices reverse the effects of transformations like rotation, scaling, or shearing.\n",
    "\n",
    "### Conditions for Invertibility\n",
    "\n",
    "For a matrix to have an inverse:\n",
    "1. It must be square (same number of rows and columns).\n",
    "2. Its determinant must be non-zero.\n",
    "\n",
    "If a matrix's determinant is zero, it is said to be **singular**, and no inverse exists.\n",
    "\n",
    "### Formula for the Inverse of a 2x2 Matrix\n",
    "\n",
    "For a 2x2 matrix $ A $:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If $ \\text{det}(A) = ad - bc \\neq 0 $, the inverse $ A^{-1} $ is given by:\n",
    "\n",
    "$$\n",
    "A^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Example: Inverse of a 2x2 Matrix\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "First, compute the determinant:\n",
    "\n",
    "$$\n",
    "\\text{det}(A) = (1)(4) - (2)(3) = 4 - 6 = -2\n",
    "$$\n",
    "\n",
    "Since the determinant is non-zero, the matrix is invertible. The inverse is:\n",
    "\n",
    "$$\n",
    "A^{-1} = \\frac{1}{-2} \\begin{bmatrix} 4 & -2 \\\\ -3 & 1 \\end{bmatrix} = \\begin{bmatrix} -2 & 1 \\\\ 1.5 & -0.5 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Verifying the Inverse\n",
    "\n",
    "To verify that $ A^{-1} $ is indeed the inverse, we can check the multiplication:\n",
    "\n",
    "$$\n",
    "A \\cdot A^{-1} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\cdot \\begin{bmatrix} -2 & 1 \\\\ 1.5 & -0.5 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Thus, the inverse is correct.\n",
    "\n",
    "### Applications in Real Life\n",
    "\n",
    "- **Cryptography**: Matrix inverses are used in encoding and decoding messages in certain encryption algorithms.\n",
    "- **Computer Graphics**: Inverse matrices are used in transformations to move objects in virtual 3D spaces.\n",
    "- **Physics and Engineering**: Inverse matrices solve systems of equations in circuit analysis, mechanical systems, and more.\n",
    "\n",
    "Understanding how to compute and use the inverse of a matrix is fundamental in many areas of mathematics and applied sciences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7781e649-30f5-48b2-8515-347b26db8425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2. ,  1. ],\n",
       "       [ 1.5, -0.5]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inverse of a matrix\n",
    "inverse_A = np.linalg.inv(A)\n",
    "inverse_A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cc32ca-5e10-45b0-aa9e-782cdb46f358",
   "metadata": {},
   "source": [
    "## Eigenvalues and Eigenvectors\n",
    "\n",
    "Given a square matrix $ A $, a non-zero vector $ \\textbf{v} $ is called an **eigenvector** of $ A $ if it satisfies the equation:\n",
    "\n",
    "$$\n",
    "A \\textbf{v} = \\lambda \\textbf{v}\n",
    "$$\n",
    "\n",
    "where $ \\lambda $ is a scalar known as the **eigenvalue** corresponding to the eigenvector $ \\textbf{v} $. Eigenvalues and eigenvectors provide deep insights into the properties of a matrix, particularly its linear transformations, stability, and structure.\n",
    "\n",
    "### Why Do We Need Eigenvalues and Eigenvectors?\n",
    "\n",
    "Eigenvalues and eigenvectors are powerful tools in linear algebra with many applications across mathematics, physics, engineering, and data science:\n",
    "\n",
    "- **Stability Analysis**: In systems of differential equations, eigenvalues help determine the stability of equilibria.\n",
    "- **Principal Component Analysis (PCA)**: In data science and machine learning, eigenvectors represent principal directions of variance, helping reduce dimensionality.\n",
    "- **Quantum Mechanics**: Eigenvalues and eigenvectors describe observable quantities, such as energy levels, in quantum systems.\n",
    "- **Vibration Analysis**: In mechanical and structural engineering, they help analyze natural vibration modes of systems.\n",
    "- **Graph Theory**: Eigenvectors can describe properties of networks, such as the importance of nodes.\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "The equation $A \\textbf{v} = \\lambda \\textbf{v} $ means that the matrix $ A $ transforms the vector $\\textbf{v} $ into a new vector that is simply a scaled version of $ \\textbf{v} $ itself, with the scaling factor being $\\lambda $. The eigenvector $ \\textbf{v} $ points in a direction that remains unchanged by the transformation.\n",
    "\n",
    "### How to Find Eigenvalues and Eigenvectors\n",
    "\n",
    "1. **Eigenvalues**: To find the eigenvalues $ \\lambda $ of a matrix $ A $, we solve the **characteristic equation**:\n",
    "\n",
    "$$\n",
    "\\text{det}(A - \\lambda I) = 0\n",
    "$$\n",
    "\n",
    "where $ I $ is the identity matrix, and $ \\text{det} $ denotes the determinant.\n",
    "\n",
    "2. **Eigenvectors**: Once the eigenvalues $ \\lambda $ are found, we substitute each eigenvalue into the equation $ A \\textbf{v} = \\lambda \\textbf{v} $ and solve for the eigenvector $ \\textbf{v} $.\n",
    "\n",
    "### Example: Eigenvalues and Eigenvectors of a 2x2 Matrix\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} 1 & 2 \\\\ 2 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "1. **Find the Eigenvalues**:\n",
    "   The characteristic equation is:\n",
    "\n",
    "   $$\n",
    "   \\text{det}(A - \\lambda I) = \\text{det}\\begin{bmatrix} 1 - \\lambda & 2 \\\\ 2 & 1 - \\lambda \\end{bmatrix} = 0\n",
    "   $$\n",
    "\n",
    "   Expanding the determinant:\n",
    "\n",
    "   $$\n",
    "   (1 - \\lambda)(1 - \\lambda) - (2)(2) = 0\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   (\\lambda^2 - 2\\lambda + 1) - 4 = 0\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\lambda^2 - 2\\lambda - 3 = 0\n",
    "   $$\n",
    "\n",
    "  Solving this system gives $ \\textbf{v}_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} $.\n",
    "\n",
    "Thus, the eigenvalues of $ A $ are $ \\lambda_1 = 3 $ and $ \\lambda_2 = -1 $, and the corresponding eigenvectors are $ \\textbf{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $ and $ \\textbf{v}_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} $.\n",
    "\n",
    "### Applications in Real Life\n",
    "\n",
    "- **Machine Learning**: Eigenvectors are used in PCA to reduce dimensionality and extract important features.\n",
    "- **Vibration Analysis**: In engineering, eigenvalues describe the natural frequencies at which structures vibrate.\n",
    "- **Quantum Mechanics**: In quantum systems, eigenvalues represent measurable quantities like energy, while eigenvectors describe the state of the system.\n",
    "- **Markov Chains**: In probability, eigenvectors of the transition matrix represent the steady-state distributions.\n",
    "\n",
    "Eigenvalues and eigenvectors reveal fundamental properties of linear transformations, making them essential tools in a wide variety of scientific and engineering applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d04dbf14-55fb-409c-8ad1-fd1faa8438c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3., -1.]),\n",
       " array([[ 0.70710678, -0.70710678],\n",
       "        [ 0.70710678,  0.70710678]]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a matrix\n",
    "C = np.array([[1, 2], [2, 1]])\n",
    "\n",
    "# Eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(C)\n",
    "eigenvalues, eigenvectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a921e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Important concepts in Linear Algebra \n",
    "\n",
    "## Linear Transformations\n",
    "Linear transformations are functions that map vectors from one vector space to another while preserving the operations of vector addition and scalar multiplication. They play a key role in machine learning, especially in neural networks where layers of transformations are applied to input data.\n",
    "\n",
    "### Properties of Linear Transformations:\n",
    "- For a linear transformation $ T $, the following properties hold:\n",
    "  - **Additivity**: $ T(\\mathbf{v}_1 + \\mathbf{v}_2) = T(\\mathbf{v}_1) + T(\\mathbf{v}_2) $\n",
    "  - **Homogeneity**: $ T(\\alpha \\mathbf{v}) = \\alpha T(\\mathbf{v}) $, where $ \\alpha $ is a scalar.\n",
    "\n",
    "Linear transformations can be represented as matrix multiplications. For example, applying a transformation $ T $ to a vector $ \\mathbf{v} $ is equivalent to multiplying a matrix $ A $ by $ \\mathbf{v} $.\n",
    "\n",
    "### Applications in Machine Learning:\n",
    "- Linear transformations are the foundation for layers in neural networks, where inputs are transformed through matrices (weights) and passed through activation functions.\n",
    "- Data preprocessing often involves linear transformations such as scaling, rotation, and projection onto lower-dimensional spaces.\n",
    "\n",
    "### Python Example: Linear Transformation with a Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef087080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed vector: [2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a linear transformation matrix\n",
    "T = np.array([[2, 0], [0, 3]])\n",
    "\n",
    "# Input vector\n",
    "v = np.array([1, 1])\n",
    "\n",
    "# Apply the transformation\n",
    "transformed_v = np.dot(T, v)\n",
    "print(\"Transformed vector:\", transformed_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afccd301",
   "metadata": {},
   "source": [
    "## Norms\n",
    "A **norm** is a measure of the length or magnitude of a vector. In machine learning, norms are used to quantify the size of vectors, which is essential in optimization problems, regularization, and measuring distances between data points.\n",
    "\n",
    "### Common Norms:\n",
    "1. **L1 Norm (Manhattan Distance)**:\n",
    "   - This norm is the sum of the absolute values of the vector's components:\n",
    "     $$\n",
    "     \\| \\mathbf{v} \\|_1 = \\sum_{i=1}^{n} |v_i|\n",
    "     $$\n",
    "   - **Application**: Lasso regularization uses the L1 norm to encourage sparsity in machine learning models by penalizing large coefficients.\n",
    "\n",
    "2. **L2 Norm (Euclidean Distance)**:\n",
    "   - This norm is the square root of the sum of the squared components of the vector:\n",
    "     $$\n",
    "     \\| \\mathbf{v} \\|_2 = \\sqrt{\\sum_{i=1}^{n} v_i^2}\n",
    "     $$\n",
    "   - **Application**: Ridge regression uses the L2 norm for regularization, penalizing large weights to avoid overfitting.\n",
    "\n",
    "### Python Example: Calculating Norms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4f87717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Norm: 7.0\n",
      "L2 Norm: 5.0\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "# Define a vector\n",
    "v = np.array([3, 4])\n",
    "\n",
    "# Calculate L1 and L2 norms\n",
    "l1_norm = norm(v, 1)\n",
    "l2_norm = norm(v, 2)\n",
    "\n",
    "print(f\"L1 Norm: {l1_norm}\")\n",
    "print(f\"L2 Norm: {l2_norm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a15b1",
   "metadata": {},
   "source": [
    "## Orthogonal and Orthonormal Matrices\n",
    "A matrix is **orthogonal** if its transpose is equal to its inverse, i.e., $ Q^T Q = I $, where $ I $ is the identity matrix. This property ensures that the matrix preserves the length and angles between vectors, making orthogonal matrices useful in many machine learning algorithms, such as PCA.\n",
    "\n",
    "An **orthonormal** matrix has columns that are both orthogonal and of unit length.\n",
    "\n",
    "### Properties:\n",
    "- **Length-preserving**: If $ Q $ is orthogonal, $ \\| Q \\mathbf{v} \\| = \\| \\mathbf{v} \\| $.\n",
    "- **Angle-preserving**: Orthogonal matrices preserve the angles between vectors.\n",
    "\n",
    "### Applications in Machine Learning:\n",
    "- **Principal Component Analysis (PCA)** uses orthogonal matrices to project data onto lower-dimensional subspaces while preserving as much variance as possible.\n",
    "- **QR Decomposition**, used in solving linear systems and least squares problems, relies on orthogonal matrices.\n",
    "\n",
    "### Python Example: Orthogonal Matrix Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "00f64dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Q orthogonal? True\n"
     ]
    }
   ],
   "source": [
    "Q = np.array([[1, 0], [0, -1]])\n",
    "\n",
    "# Check if Q is orthogonal\n",
    "is_orthogonal = np.allclose(np.dot(Q.T, Q), np.eye(2))\n",
    "print(f\"Is Q orthogonal? {is_orthogonal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22256ac1",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (SVD)\n",
    "**Singular Value Decomposition (SVD)** is a matrix factorization technique that decomposes any matrix \\( A \\) into three matrices:\n",
    "$$\n",
    "A = U \\Sigma V^T\n",
    "$$\n",
    "Where:\n",
    "- $ U $ is an orthogonal matrix.\n",
    "- $ \\Sigma $ is a diagonal matrix of singular values (non-negative values).\n",
    "- $ V^T $ is the transpose of an orthogonal matrix \\( V \\).\n",
    "\n",
    "### Applications in Machine Learning:\n",
    "- **Dimensionality Reduction**: SVD is used in techniques like PCA for reducing the dimensionality of data while retaining the most important information.\n",
    "- **Latent Semantic Analysis (LSA)**: In natural language processing, SVD is used to identify patterns in the relationships between terms and documents.\n",
    "\n",
    "### Python Example: SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5b807b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U Matrix:\n",
      " [[-0.2298477   0.88346102  0.40824829]\n",
      " [-0.52474482  0.24078249 -0.81649658]\n",
      " [-0.81964194 -0.40189603  0.40824829]]\n",
      "Sigma (Singular Values):\n",
      " [9.52551809 0.51430058]\n",
      "V Transpose Matrix:\n",
      " [[-0.61962948 -0.78489445]\n",
      " [-0.78489445  0.61962948]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import svd\n",
    "\n",
    "# Define a matrix\n",
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# Perform SVD\n",
    "U, Sigma, Vt = svd(A)\n",
    "\n",
    "print(\"U Matrix:\\n\", U)\n",
    "print(\"Sigma (Singular Values):\\n\", Sigma)\n",
    "print(\"V Transpose Matrix:\\n\", Vt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9cad84",
   "metadata": {},
   "source": [
    "## Rank of a Matrix\n",
    "The **rank** of a matrix is the number of linearly independent rows or columns in the matrix. It indicates the maximum number of independent vectors that can be extracted from the matrix.\n",
    "\n",
    "### Applications in Machine Learning:\n",
    "- **Linear Systems**: The rank helps determine whether a system of linear equations has a unique solution. A matrix with full rank implies a unique solution, while a lower-rank matrix may indicate an under-determined system.\n",
    "- **Data Compression**: In dimensionality reduction techniques like PCA, the rank of a matrix tells us how many dimensions are truly needed to represent the data.\n",
    "\n",
    "### Python Example: Matrix Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98543207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of the matrix: 2\n"
     ]
    }
   ],
   "source": [
    "# Define a matrix\n",
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Compute the rank\n",
    "rank = np.linalg.matrix_rank(A)\n",
    "print(f\"Rank of the matrix: {rank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25046b0f",
   "metadata": {},
   "source": [
    "## Projections\n",
    "A **projection** is a linear transformation that maps a vector onto a subspace. In machine learning, projections are used for dimensionality reduction, where data is projected onto a lower-dimensional space.\n",
    "\n",
    "### Applications in Machine Learning:\n",
    "- **Principal Component Analysis (PCA)** projects data onto principal components, which are the directions of maximum variance, to reduce the dimensionality of the dataset.\n",
    "- **Feature Engineering**: Projections can help remove irrelevant features by projecting data onto a subspace where only important features remain.\n",
    "\n",
    "### Python Example: Projection of a Vector onto a Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bbfe0e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projection of v onto u: [2. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Define a vector and a line (unit vector)\n",
    "v = np.array([2, 3])\n",
    "u = np.array([1, 0])  # Unit vector along x-axis\n",
    "\n",
    "# Projection of v onto u\n",
    "projection = (np.dot(v, u) / np.dot(u, u)) * u\n",
    "print(\"Projection of v onto u:\", projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf60ec7",
   "metadata": {},
   "source": [
    "## QR Decomposition\n",
    "**QR Decomposition** is a method of decomposing a matrix into two components:\n",
    "- $ Q $, an orthogonal matrix.\n",
    "- $ R $, an upper triangular matrix.\n",
    "\n",
    "This is useful for solving linear systems and performing least squares optimization.\n",
    "\n",
    "### Applications in Machine Learning:\n",
    "- **Solving Linear Systems**: QR decomposition is used to solve systems of linear equations by reducing the complexity of matrix inversion.\n",
    "- **Optimization**: In regression analysis, QR decomposition is used to compute the least squares solution to overdetermined systems (when there are more equations than unknowns).\n",
    "\n",
    "### Python Example: QR Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28e3299f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Matrix:\n",
      " [[-0.85714286  0.39428571  0.33142857]\n",
      " [-0.42857143 -0.90285714 -0.03428571]\n",
      " [ 0.28571429 -0.17142857  0.94285714]]\n",
      "R Matrix:\n",
      " [[ -14.  -21.   14.]\n",
      " [   0. -175.   70.]\n",
      " [   0.    0.  -35.]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import qr\n",
    "\n",
    "# Define a matrix\n",
    "A = np.array([[12, -51, 4], [6, 167, -68], [-4, 24, -41]])\n",
    "\n",
    "# Perform QR decomposition\n",
    "Q, R = qr(A)\n",
    "\n",
    "print(\"Q Matrix:\\n\", Q)\n",
    "print(\"R Matrix:\\n\", R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee46721",
   "metadata": {},
   "source": [
    "# Linear Regression on Housing Dataset using Linear Algebra\n",
    "\n",
    "Linear regression is a fundamental method for predicting a target variable based on one or more input features. In this notebook, we will use the Boston housing dataset to demonstrate linear regression. We will perform the regression both with Scikit-learn's built-in functionality and by using linear algebra operations.\n",
    "\n",
    "Linear regression tries to find the best-fit line for the data using the equation:  \n",
    "$$ \\hat{y} = X \\beta + \\epsilon $$  \n",
    "where:\n",
    "- $X$ is the matrix of input features,\n",
    "- $\\beta$ is the vector of coefficients,\n",
    "- $\\hat{y}$ is the predicted output,\n",
    "- $\\epsilon$ is the error term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4e477705-7aae-413d-a69d-0b39a0a011dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  MedHouseVal  \n",
       "0    -122.23        4.526  \n",
       "1    -122.22        3.585  \n",
       "2    -122.24        3.521  \n",
       "3    -122.25        3.413  \n",
       "4    -122.25        3.422  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# Load the California housing dataset\n",
    "california = fetch_california_housing()\n",
    "data = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "data['MedHouseVal'] = california.target  # MedHouseVal is the target (house prices)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f39059",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "We will be using the Boston housing dataset, which contains information on housing prices in Boston suburbs. The target variable is the median value of owner-occupied homes (`MEDV`), and there are 13 input features such as the percentage of lower status of the population (`LSTAT`), average number of rooms per dwelling (`RM`), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e1448b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X = data.drop('MedHouseVal', axis=1)  # Features\n",
    "y = data['MedHouseVal']  # Target variable\n",
    "\n",
    "# Standardize the features for better performance\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91bd7e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Linear Regression using Linear Algebra\n",
    "\n",
    "The formula for linear regression is based on the normal equation:\n",
    "$$ \\hat{\\beta} = (X^T X)^{-1} X^T y $$\n",
    "Where:\n",
    "- $X$ is the matrix of input features (with a column of ones added for the intercept),\n",
    "- $y$ is the target vector,\n",
    "- $\\hat{\\beta}$ is the vector of coefficients that we are trying to solve for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c9da290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients (beta_hat):\n",
      "[ 2.06786231  0.85238169  0.12238224 -0.30511591  0.37113188 -0.00229841\n",
      " -0.03662363 -0.89663505 -0.86892682]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column of ones to X_train for the intercept term\n",
    "X_b = np.c_[np.ones((X_train.shape[0], 1)), X_train]  # Add bias term (intercept)\n",
    "\n",
    "# Calculate the coefficients using the normal equation\n",
    "beta_hat = np.dot(inv(np.dot(X_b.T, X_b)), np.dot(X_b.T, y_train))\n",
    "\n",
    "# Print the coefficients\n",
    "print(f\"Coefficients (beta_hat):\\n{beta_hat}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ee8bc7",
   "metadata": {},
   "source": [
    "## Predictions and Evaluation\n",
    "\n",
    "Now that we have the coefficients, we can use them to make predictions on the test set. We'll also evaluate the model's performance by calculating the Mean Squared Error (MSE) on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "07f44da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.555891598695247\n"
     ]
    }
   ],
   "source": [
    "# Prepare X_test for predictions (add intercept term)\n",
    "X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "\n",
    "# Make predictions\n",
    "y_pred = np.dot(X_test_b, beta_hat)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed06d8",
   "metadata": {},
   "source": [
    "## Comparison with Scikit-learn's Linear Regression\n",
    "\n",
    "Let's compare our manually computed linear regression model with Scikit-learn's built-in linear regression to ensure that the results match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff815a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (Scikit-learn): 0.5558915986952441\n",
      "Scikit-learn coefficients:\n",
      "[ 2.06786231  0.85238169  0.12238224 -0.30511591  0.37113188 -0.00229841\n",
      " -0.03662363 -0.89663505 -0.86892682]\n",
      "\n",
      "Manual coefficients (linear algebra):\n",
      "[ 2.06786231  0.85238169  0.12238224 -0.30511591  0.37113188 -0.00229841\n",
      " -0.03662363 -0.89663505 -0.86892682]\n"
     ]
    }
   ],
   "source": [
    "# Import Scikit-learn's LinearRegression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Fit the model using Scikit-learn\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_sklearn = lin_reg.predict(X_test)\n",
    "\n",
    "# Calculate the Mean Squared Error for comparison\n",
    "mse_sklearn = mean_squared_error(y_test, y_pred_sklearn)\n",
    "print(f\"Mean Squared Error (Scikit-learn): {mse_sklearn}\")\n",
    "\n",
    "# Compare coefficients\n",
    "print(f\"Scikit-learn coefficients:\\n{np.r_[lin_reg.intercept_, lin_reg.coef_]}\\n\")\n",
    "print(f\"Manual coefficients (linear algebra):\\n{beta_hat}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
